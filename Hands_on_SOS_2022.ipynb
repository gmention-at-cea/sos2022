{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands-on_SOS_2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**<h1><font color=\"darkred\">Hands on session on fundamental of statistics</font></h1>**\n",
        "\n",
        "**Welcome to this session**\n",
        "\n",
        "We will use this notebook to practice together with statistics and data anlaysis for **SOS 2022**.\n"
      ],
      "metadata": {
        "id": "GZmoqL0RgVtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">My own big summary of statistics and probabilities</font>\n",
        "\n",
        "**Everything about probability** and statistics is fundamentally about a random outcome (*i.e.* on which you don't have full control), and the set of possibilities it should belong to.\n"
      ],
      "metadata": {
        "id": "yTFpcY0nZDSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**To analyze things** in terms of probability and statistics is to locate an occurrence in terms of its weight in a given set of possible outcomes.\n",
        "\n",
        "**To get a meaning** of the part in the whole, you must define the whole. And everything is there. Remember. Everything is there. Class of distributions, model definition, model and data adequacy, classification and so on. The fundamental question is: what are the potential outcomes?\n",
        "\n",
        "**Whatever flavour** you like, Bayesian, Frequentists, Machine Learning setups have a prioris. Define a sample space, a broad class of possibilities, and find a more specific one in this set to explain the observed data collection to gain insight or predict new outcomes. Starting from a class of probabilities is already an a priori on the problem.\n",
        "\n",
        "Enough of generic, but fundamental concepts, **let's get down to business**!!!\n"
      ],
      "metadata": {
        "id": "-mpwYRZQI0hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Using Colaboratory</font>\n",
        "\n",
        "Use *Google Chrome* browser and Google Colaboratory.\n"
      ],
      "metadata": {
        "id": "7hSRcmuMZf46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The practice environement - Colaboratory\n",
        "**Colaboratory** = python notebooks (Jupyter) on Google servers. Convenient to share, teach, provide demonstrations... Only suited for small projects or small code. Otherwise use development environment on your computer (Atom, Pycharm, Spyder, Sublime Text,...). You can still develop large python classes and scripts (`.py` files) upload them on Google Drive and call them in your notebooks...\n",
        "\n",
        "Main **purpose** of Colaboratory is thus: **work together and share**.\n",
        "Advantages:\n",
        "* Write and execute python code in your web browser (even on smartphones, tablets,...)\n",
        "* Create/upload and share notebooks\n",
        "* Import and save notebooks from/to Google Drive or GitHub\n",
        "* Plus a lot of extras: import export datas (pandas, tensorflow, keras, use provided GPU or TPU by Google) etc..\n",
        "\n",
        "**A python notebook** is a file (usually \".ipynb\") with both `TEXT` and `CODE`, splitted in separate cells which forms the notebook. `TEXT` cells help describe the frame and the computations for demonstration purposes. `CODE` cells are executable and can share variables in the notebook workspace environement (kernel) and produce outputs (printing, tables, graphics). That's all to start with.\n",
        "\n",
        "<font color=\"chocolate\">**NOTES**</font>: \n",
        "1. I recommend to **switch the Colab language to English**. For this, click on the Colab `Help` from the menubar and select the last line telling you to `switch to English`.\n",
        "\n",
        "2. To work with this notebook, you have to save it in your Google Drive. Go to \"File > Save a Copy in Drive\". You are done. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amGeZW5IImMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **<font color=\"darkred\">Outline</font>**\n",
        "\n",
        "<font color=\"gray\">We will cover the following:</font>\n",
        "<a name=\"index\"></a>\n",
        "1. [Probabilities](#probabilities): most standard ones and properties.\n",
        "2. [Sampling](#sampling): getting outcomes.\n",
        "3. [Statistical inference](#inference): from generic class of models to a specific one.\n",
        "\n",
        "<font color=\"gray\">Note: below the icon [ðŸ”¼](#index) moves back to this index.</font>"
      ],
      "metadata": {
        "id": "dDV9rUEHkDAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- \n",
        "\n",
        "# <font color=\"darkred\">**Introducing yourself**</font>"
      ],
      "metadata": {
        "id": "7JQcMPkES9tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today's session. You are in the room, I'm only on the screen...\n",
        "\n",
        "To manage this session I'd like you to fill in this form, such that I can identify your contributions in this session and help you when needed."
      ],
      "metadata": {
        "id": "3-zBRt6Yth1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please first <font color=\"chocolate\">**introduce yourself**</font> by filling out the following forms and **evaluate the cell once** you have completed all the fields, by cliking on the \"play button\" on the top left of the cell or by hitting `SHIFT`+`RETURN` on your keyboard."
      ],
      "metadata": {
        "id": "DMm0iAaVTDbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt3P0sriZWnf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import display, HTML\n",
        "disp = lambda x: display(HTML(x))\n",
        "\n",
        "#@markdown $\\color{darkorange}{\\rangle}\\;$ **<font color=\"darkred\">Your ID</font>**\n",
        "first_name = \"\" #@param {type:\"string\"}\n",
        "family_name = \"\" #@param {type:\"string\"}\n",
        "email_address = \"\" #@param {type:\"string\"}\n",
        "research_field = \"Particle Physics\" #@param [\"Particle Physics\", \"Nuclear Physics\", \"Astroparticle Physics\", \"Astrophysics\", \"Cosmology\", \"Other\"]\n",
        "\n",
        "#@markdown $\\color{darkorange}{\\rangle}\\;$ Your <font color=\"darkred\">**python level**</font> in `numpy`, `scipy`, `matplotlib`\n",
        "python_level = 'Beginner' #@param [\"Beginner\", \"Intermediate\", \"Advanced\"]\n",
        "\n",
        "#@markdown $\\color{darkorange}{\\rangle}\\;$ You use python notebooks (**Jupyter**):\n",
        "notebook_use = 'Rarely' #@param [\"Rarely\", \"From time to time\", \"Often\"]\n",
        "\n",
        "\n",
        "#@markdown $\\color{darkorange}{\\rangle}\\;$ You alreay know and use <font color=\"darkred\">**Colaboratory**</font>:\n",
        "colab_use = 'No' #@param [\"No\", \"Yes\"]\n",
        "\n",
        "#TODO : curl to download code (module) on github\n",
        "#then import module for submissions\n",
        "#then call the specific submit function\n",
        "#submit_ID()\n",
        "# Possibility to upload a photo?\n",
        "import requests\n",
        "\n",
        "url = \"https://docs.google.com/forms/d/e/1FAIpQLSeMPDC0U_fChbbft-aNolvQXdaknimVsBatDW9zQAum5Qm9uA/formResponse\"\n",
        "\n",
        "form_data = {\n",
        "  \"emailAddress\": email_address.lower(),\n",
        "  \"entry.297261647\": first_name,\n",
        "  \"entry.585381984\": family_name,\n",
        "  \"entry.1427088777\": python_level,\n",
        "  \"entry.1159612919\": '; '.join([research_field, notebook_use, colab_use]),\n",
        "}\n",
        "try: \n",
        "  r = requests.post(url, data=form_data)\n",
        "except:\n",
        "  print('There seems to be an error. Please ask you teacher for help!')\n",
        "else:\n",
        "  disp('<font color=\"chocolate\"><b style=\"font-size:20pt\">Thank you!<b></font>')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# <font color=\"darkred\">**Basic module loadings**</font>\n",
        "\n",
        "Module loading. Use of:\n",
        "- `numpy`: mostly for mathematical functions & numeric arrays;\n",
        "- `scipy`: mostly `scipy.stats` & `scipy.optimize`;\n",
        "- `matplotlib`: for plotting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m0aA6a3eTHBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # handles most of the numerical work\n",
        "import scipy.stats  # implements statistical tools (PDFs, etc)\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib_inline.backend_inline\n",
        "\n",
        "#import matplotlib as mpl\n",
        "#mpl.rcParams['figure.dpi'] = 300\n",
        "\n",
        "# matplotlib related\n",
        "plt.style.use('ggplot')\n",
        "# matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n",
        "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "import matplotlib.colors as colors\n",
        "colors.TABLEAU_COLORS;"
      ],
      "metadata": {
        "id": "nXTzBQwUPhoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute to define the submit() command\n",
        "def submit(Qid, var):\n",
        "  url = \"https://docs.google.com/forms/d/e/1FAIpQLSeMPDC0U_fChbbft-aNolvQXdaknimVsBatDW9zQAum5Qm9uA/formResponse\"\n",
        "\n",
        "  form_data = {\n",
        "    \"emailAddress\": email_address.lower(),\n",
        "    \"entry.297261647\": first_name,\n",
        "    \"entry.585381984\": family_name,\n",
        "    \"entry.1159612919\": ': '.join([f'Q{Qid}', var]),\n",
        "  }\n",
        "  try: \n",
        "    r = requests.post(url, data=form_data)\n",
        "  except:\n",
        "    print('There was a problem submitting this answer.')\n",
        "  else:\n",
        "    print('Submission done.')\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "JUV2V6wdcI6I",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"darkred\">**How we work together**</font>\n",
        "\n",
        "This notebook has some wholes within which are marked with `xxx` within code cells.\n",
        "\n",
        "You are asked, prior to the cell to define the `xxx` variable with a valid python statement to answer the given problem. This statement has to be placed within single quotes (to be a string).\n",
        "\n",
        "You can execute the cell immediately after to evaluate your proposal. Once you are confident with your proposition, please evaluate the cell just after containing the `submit(Qid, xxx)` code to submit your answer.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "Imagine I ask you to compute as an exercise the square root of a given list of values: `x = [1, 2, 3]`.\n",
        "\n",
        "For this purpose you have to use the `numpy.sqrt` function from `numpy` package, here imported as `np`. You should then use `np.sqrt` to compute the square root.\n",
        "\n",
        "First: evaluate the next code cell:"
      ],
      "metadata": {
        "id": "BNZXa8iv8QWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = [1, 2, 3]"
      ],
      "metadata": {
        "id": "Q5Ulaw8M9fbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill bellow `xxx='...'` with `np.sqrt(x)` and evaluate this code cell too."
      ],
      "metadata": {
        "id": "RlZ7zqhJ-UhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The cell to provide with your answer\n",
        "Qid = 0 # the question ID\n",
        "\n",
        "# Here it the xxx variable to define\n",
        "xxx = '' # fill your answer within the quote\n",
        "# xxx = 'np.sqrt(x)' \n",
        "# xxx = [ np.sqrt(values) for values in x ]"
      ],
      "metadata": {
        "id": "aVGoWODG9iMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the expression in `xxx` variable, evaluate the next cell to get you answer to the exercise."
      ],
      "metadata": {
        "id": "_74RnDZB-ke5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code in the above xxx variable is executed here\n",
        "eval(xxx)"
      ],
      "metadata": {
        "id": "DrG-dUtW9tSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are ok with your anser, please proceed with submitting it."
      ],
      "metadata": {
        "id": "2GXBT1uV-vbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "_J1mpnQl9ngR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any question with this process?\n",
        "\n",
        "We start."
      ],
      "metadata": {
        "id": "U91CHKy8_GWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One last thing. And an important one. You can have a real time copy of my notebook I'm sharing from my Colab account:\n",
        "\n",
        "> [Link to the teacher Colab notebook](https://colab.research.google.com/drive/1zDE5rlqGUS3EKGWJZpmIZXFEIveOwLLy?usp=sharing)\n",
        "\n",
        "In this way you can check the answers and the code all along even if no more on the video sharing.\n",
        "\n",
        "And don't worry, we will provide you with the exercise notebook and the one with the correction by the end of this hands-on session."
      ],
      "metadata": {
        "id": "B_kCcX625UOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<a name=\"probabilities\"></a>\n",
        "\n",
        "# <font color=\"darkred\">**I. Probability**</font> [ðŸ”¼](#index)\n",
        "\n",
        "Before doing statistics and inference, we need to set up probabilities. We start with usual ones:\n",
        "- Binomial ${\\cal B}(N,p)$: `scipy.stats.binom`\n",
        "- Poisson ${\\cal P}(\\lambda)$: `scipy.stats.poisson`\n",
        "- Normal ${\\cal N}(\\mu,\\sigma^2)$: `scipy.stats.norm`\n",
        "- Chi square $\\chi^2(n_{\\rm dof})$: `scipy.stats.chi2`"
      ],
      "metadata": {
        "id": "nuSKI5pNTP7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the [statistics lectures of R. Madar](https://indico.in2p3.fr/event/26179/timetable/#1-basic-concepts-of-statistics), you have seen a couple of **distributions** and properties.\n",
        "\n",
        "**Probabilities** are used to model the **random errors** from the measurements in Physics experiment.  \n",
        "Two basic nature of errors: **continuous** and **discrete**."
      ],
      "metadata": {
        "id": "q98fwoVgYqWx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haaIOyBvcwQh"
      },
      "source": [
        "Distribution probabilities may be found in `scipy.stats`. \n",
        "For each probability distribution, you can access  to *METHODS*:\n",
        "> * Probability density function (PDF, continuous case) or mass function (PMF, discrete case): `pdf` or `pmf`  \n",
        "> * Cumulative distribution function (CDF): `cdf`  \n",
        "> * Quantile function (functional inverse of the CDF. Percent point function): ` ppf`  \n",
        "> * Median, mean, variance, standard deviation: `median`, `mean`, `var`, `std`  \n",
        "> * Random variates: `rvs`  \n",
        "\n",
        "For each of these methods, you might provide some arguments:\n",
        "- `loc=...` to specify the x location of the distribution;\n",
        "- `scale=...` to stretch the distribution on x axis;\n",
        "- `size=...` in conjunction with `rvs` to generate a sample of a given size.\n",
        "\n",
        "To get help, you can write this in the notebook `scipy.stats.norm?` and hit `[SHIFT]`+`[RETURN]` on your keyboard or execute the cell with the 'Play' icon, `(â–¶)`, on the left of the cell code.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scipy.stats.norm?"
      ],
      "metadata": {
        "id": "hbPxCXDMZBdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or just start writing `scipy.stats.norm(` with the cursor being after the opening parenthesis and hit `[TAB]` on your keyboard \n",
        "\n",
        "<font color=\"gray\">(NOTE: for this to work, please ensure that in `Tools > Settings > Editor` the option \"Automatically trigger code completions\" is unchecked)</font>"
      ],
      "metadata": {
        "id": "mdh_y5I7S5L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scipy.stats.norm()"
      ],
      "metadata": {
        "id": "OfotxWv5TzQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Tabulated distributions - howtos</font>\n",
        "\n",
        "<font color=\"chocolate\">*Objectives:*</font>\n",
        "Plotting probability density or mass functions of standard distributions\n",
        "\n",
        "\n",
        "First, you want to know the available distributions?"
      ],
      "metadata": {
        "id": "lvK7s9mKZdLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hit TAB key in the following line after the dot of stats. You should see a\n",
        "# popup scrolling window \n",
        "# Note: If not, check in the menu of Colab : Tools > Setting > Editor\n",
        "# option \"Automatically trigger code completions\" should be unchecked\n",
        "# Save the setting changes if any. \n",
        "scipy.stats."
      ],
      "metadata": {
        "id": "QNmymCaDZjdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oYeRhhIh4VQ"
      },
      "source": [
        "Alternatively, you can use the `scipy.stats` [documentation](https://docs.scipy.org/doc/scipy/reference/stats.html) to access the descriptions with a better display formatting... Up to you..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Plotting distributions</font>\n",
        "\n",
        "<font color=\"chocolate\">*Objectives:*</font> discover `scipy.stats` package. Plot probability distributions."
      ],
      "metadata": {
        "id": "zy8LAMFoP3Po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xvals = np.linspace(-5, 5, 250) # an array of 250 points between -5 and 5 which will be used for plotting\n",
        "means = [0.0, -1.0, 2.0]   # a few mean values to plot - location parameter"
      ],
      "metadata": {
        "id": "u5D7gf74TRJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 1\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "9yXchkwCrXCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mean in means:\n",
        "    yvals = eval(xxx) # compute the y-values for each x\n",
        "    plt.plot(xvals, yvals, label=f'mean={mean:>-2g}', linewidth=2) # draw the plots\n",
        "\n",
        "# Some plotting adjustements\n",
        "plt.xlim(-5, 7)    # adjust the x range\n",
        "plt.ylim(0, 0.5)   # adjust the y range\n",
        "plt.grid(True)     # draw grid lines on the plot\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$f_X(x)$') # Standard notation for probability density function f_X\n",
        "plt.title('Normal distributions')\n",
        "plt.legend(facecolor='white');"
      ],
      "metadata": {
        "id": "YgsrkKcKrkSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are confident with your answer, please submit with executing next cell."
      ],
      "metadata": {
        "id": "xJ-bMJCBsJ7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "Z9sybp-JsJCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now repeat with different widths:"
      ],
      "metadata": {
        "id": "pgBwHeNC0dLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmas = [1.0, 2.0, 3.0]   # a few sigma values to plot"
      ],
      "metadata": {
        "id": "ZGCpg_ub0c4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make similar plots as above for the provided sigma values and zero mean   "
      ],
      "metadata": {
        "id": "yyokHZj703ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 2\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "xxGky55esnsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sigma in sigmas:\n",
        "  yvals = eval(xxx) # compute the y-values for each x\n",
        "  plt.plot(xvals, yvals, label=f'sigma={sigma:>3g}', linewidth=2)\n",
        "\n",
        "# Some plotting adjustements\n",
        "plt.xlim(-5, 5)    # adjust the x range\n",
        "plt.ylim(0, 0.5)   # adjust the y range\n",
        "plt.grid(True)     # draw grid lines on the plot\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$F_X(x)$') # Standard notation for probability cumulative function F_X\n",
        "plt.title('Normal distributions')\n",
        "plt.legend(facecolor='white');"
      ],
      "metadata": {
        "id": "hMKNQZQO0w2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are confident with your answer, please submit with executing next cell."
      ],
      "metadata": {
        "id": "a2b0adtxtKNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "mgtBYsl1tKNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X variable should contain the normal distribution from stats package.\n",
        "mu = 5\n",
        "sigma = 1.2\n",
        "X = scipy.stats.norm(loc=mu, scale=sigma)\n",
        "\n",
        "x = np.linspace(0,10,1000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,3))\n",
        "ax1.plot(x, X.pdf(x))\n",
        "ax1.set_xlabel('$x$')\n",
        "ax1.set_ylabel('PDF: $f_X(x)$')\n",
        "ax1.set_title('Density distribution function')\n",
        "\n",
        "ax2.plot(x, X.cdf(x))\n",
        "ax2.set_xlabel('$x$')\n",
        "ax2.set_ylabel('CDF: $F_X(x)$')\n",
        "ax2.set_title('Cumulative distribution function')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cKYYU1OFtyDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantiles** of a probability distribution $f_X$ are $x_\\alpha$ values such that $F_X(x_\\alpha)=\\int_{-\\infty}^{x_\\alpha}f_X(u)\\;du=\\alpha$\n",
        "\n",
        "so that\n",
        "\n",
        "$$x_{\\alpha} = F_X^{-1}(\\alpha)$$\n",
        "\n",
        "Plot the quantile distribution of $X$ defined previously (normal distribution with mean `mu` and standard deviation `sigma`)."
      ],
      "metadata": {
        "id": "7pxzrg3KHMiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 3\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "pVYU3NudHwXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.linspace(0, 1, 1_000)\n",
        "  \n",
        "plt.plot(p, eval(xxx))\n",
        "  \n",
        "plt.xlabel('p')\n",
        "plt.ylabel('$Q_X(p)=F_X^{-1}(p)$')\n",
        "plt.title('Quantile');"
      ],
      "metadata": {
        "id": "gjxY2z5kHR-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "00q8-EfqIAzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { run: \"auto\", vertical-output: true }\n",
        "z0 = -0.7\n",
        "\n",
        "# To have fun with sliders, uncomment the line below\n",
        "#z0 = -0.6 #@param {type:\"slider\", min:-3, max:3, step:0.1}\n",
        "\n",
        "# or you can use the \"Insert > Add a form field\" menu of Google Colab.\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(4,8))\n",
        "plt.subplots_adjust(hspace=.5)\n",
        "x = np.arange(-3,3,0.001)\n",
        "mu = 0\n",
        "sigma =1\n",
        "\n",
        "z = x[x<z0]\n",
        "f = scipy.stats.norm.pdf\n",
        "f0 = f(z0, mu, sigma)\n",
        "# EX\n",
        "ax1.plot(x, f(x, mu, sigma))\n",
        "# EX\n",
        "ax1.fill_between(z, 0, f(z, mu, sigma), alpha=0.25, color='k')\n",
        "ax1.plot(z0, f0, 'ko')\n",
        "ax1.plot([z0, z0],[0, f(z0, mu, sigma)], 'k--')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('$f_X(x)$')\n",
        "ax1.set_title('Density')\n",
        "\n",
        "F = scipy.stats.norm.cdf\n",
        "F0 = F(z0, mu, sigma)\n",
        "# EX\n",
        "ax2.plot(x, F(x, mu, sigma))\n",
        "# EX\n",
        "ax2.plot(z0, F0, 'ko')\n",
        "ax2.plot([z0, z0],[0, F0], 'k--')\n",
        "ax2.plot([-3, z0],[F0, F0], 'k:')\n",
        "ax2.set_xlabel('x')\n",
        "ax2.set_ylabel('$F_X(x)$')\n",
        "ax2.set_title('Cumulative')\n",
        "\n",
        "Q = scipy.stats.norm.ppf\n",
        "p = np.linspace(0, 1, 1_000)\n",
        "# EX\n",
        "ax3.plot(p, Q(p, mu, sigma))\n",
        "# EX\n",
        "ax3.plot(F0, z0, 'ko')\n",
        "ax3.plot([0, F0], [z0, z0], 'k--')\n",
        "ax3.plot([F0, F0], [-3, z0], 'k:')\n",
        "ax3.set_xlabel('p')\n",
        "ax3.set_ylabel('$Q_X(p)=F_X^{-1}(p)$')\n",
        "ax3.set_title('Quantile')\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kgpfe-KtuuPl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-sided probability content\n",
        "xvals = np.linspace(-5, 5, 100)\n",
        "yvals = [ scipy.stats.norm.pdf(xval) for xval in xvals ]\n",
        "fig = plt.figure()\n",
        "plt.plot(xvals, yvals)\n",
        "plt.ylim(0, 0.5)\n",
        "plt.title(\"Cumulative normal distribution\\n 1-sided probability content\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f_X(x)')\n",
        "\n",
        "# Now fill in part of the distribution\n",
        "xup = 1\n",
        "shaded_xvals = np.linspace(-5, xup, 100)\n",
        "shaded_yvals = [ scipy.stats.norm.pdf(xval) for xval in shaded_xvals ]\n",
        "plt.fill_between(shaded_xvals, shaded_yvals, alpha=.25, color='tab:red');"
      ],
      "metadata": {
        "id": "d7OyGLHXvwMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the probability content associated to the shaded region?"
      ],
      "metadata": {
        "id": "uJzzVOblKVVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 4\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "b0R41rWpKZYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(xxx)"
      ],
      "metadata": {
        "id": "hQd6sUltKg_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "x0tusmBfKmhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two-sided probability content\n",
        "fig = plt.figure()\n",
        "plt.plot(xvals, scipy.stats.norm.pdf(xvals)) # Can use the function(xvals) syntax for simplicity, same as [ function(xval) for xval in xvals ]\n",
        "plt.ylim(0,0.5)\n",
        "plt.title(\"Two-sided probability content\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f_X(x)')\n",
        "\n",
        "xlim = 1\n",
        "shaded_xvals = np.linspace(-xlim,xlim,100)\n",
        "plt.fill_between(shaded_xvals, scipy.stats.norm.pdf(shaded_xvals), alpha=0.25, color='tab:red');"
      ],
      "metadata": {
        "id": "rs-S2GcrBF1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the probability content associated to the shaded region?"
      ],
      "metadata": {
        "id": "5TpgWzfZK-W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 5\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "O3kX8UvYKt34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(xxx)"
      ],
      "metadata": {
        "id": "b80KIzJQM0aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "T6F68u-2MwW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute for the 5 bounding values: [1, 2, 3, 4, 5]\n",
        "1. The lower one-sided probability content: $\\int_{-\\infty}^{\\rm bound} f_X(u)\\;du$"
      ],
      "metadata": {
        "id": "rVtmdGkKLKYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bounds = [1, 2, 3, 4, 5]"
      ],
      "metadata": {
        "id": "rtLYGkXOLsTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 6\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "JSPqV041Li-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_sided = eval(xxx)\n",
        "one_sided"
      ],
      "metadata": {
        "id": "c1VENW8dAp_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "bKiIZlTgNJmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The upper one-sided probability content: $\\int_{\\rm bound}^{+\\infty} f_X(u)\\;du$"
      ],
      "metadata": {
        "id": "qOoS8lakL8JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 7\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "u4hpGd5PMCIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_sided_tail = eval(xxx)\n",
        "one_sided_tail"
      ],
      "metadata": {
        "id": "UgiVVNsBL1n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "UqSZWX6bNM37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The two sided probability content: $\\int_{-{\\rm bound}}^{+{\\rm bound}}f_{X}(u)\\; du$ :"
      ],
      "metadata": {
        "id": "iOuDBD8ZMQmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 8\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "4Hb9A-AbMkFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_sided = eval(xxx)\n",
        "two_sided"
      ],
      "metadata": {
        "id": "bDHC8jvoMNLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "45TcsS_p15Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The probability content in the outer tails $\\int_{-\\infty}^{-{\\rm bound}}f_{X}(u)\\; du + \\int_{+{\\rm bound}}^{\\infty} f_X(u)\\; du$ :"
      ],
      "metadata": {
        "id": "dNDHAH6SN1tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 9\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "zbAAaxkSOHmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_sided_tail = eval(xxx)\n",
        "two_sided_tail"
      ],
      "metadata": {
        "id": "_IN1-NgZMs_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "aHEsb4byOUD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the results:\n",
        "import pandas as pd\n",
        "data = np.array(np.round([ bounds, one_sided, one_sided_tail, two_sided, two_sided_tail ], 4)).T\n",
        "labels = [ 'Bound', '1-sided', '1-tail', '2-sided', '2-tail' ]\n",
        "pd.DataFrame(data, columns=labels)"
      ],
      "metadata": {
        "id": "FMuubG2MOOZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find back the usual table of the standard normal distribution and associated probaility content from 1 to 5$\\sigma$."
      ],
      "metadata": {
        "id": "tElfxzkDAcsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now focus on the $\\chi^2$ distribution, another important distribution for confidence intervals we will see later.\n",
        "\n",
        "Plot the $\\chi^2$ PDF for 1, 2, 3, 4, 5 degrees of freedom."
      ],
      "metadata": {
        "id": "1UV42BMw2Whu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 10\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "lOzZnItBXR6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(0, 15, 1_000)\n",
        "for ndof in [1, 2, 3, 4, 5]:\n",
        "\n",
        "  plt.plot(x, eval(xxx), label=f'ndof = {ndof}')\n",
        "\n",
        "  \n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$f_{\\chi^2}(x)$')\n",
        "plt.axis([0, 15, 0, .5])\n",
        "plt.legend(facecolor= 'white');"
      ],
      "metadata": {
        "id": "Jp8caFjDT8B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "qRaLIzQYXWAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Compute quantiles for 90%, 95%, 99% probability content and for 1 and 2 degrees of freedom."
      ],
      "metadata": {
        "id": "7Kx-iZepTzuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 11\n",
        "xxx1 = ''\n",
        "xxx2 = ''"
      ],
      "metadata": {
        "id": "VWu2VCVwXol2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bounds = [.9, .95, .99]\n",
        "x2_1 = eval(xxx1)\n",
        "x2_2 = eval(xxx2)\n",
        "\n",
        "data = np.array(np.round([ bounds, x2_1, x2_2], 2)).T\n",
        "labels = [ 'CL', '1 dof', '2 dof' ]\n",
        "pd.DataFrame(data, columns=labels)"
      ],
      "metadata": {
        "id": "YXmEjQb5Te4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, '; '.join([xxx1, xxx2]))"
      ],
      "metadata": {
        "id": "yjYgQQ9EXyNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same for \"1, 2, 3, 4 and 5$\\sigma$\" probability content (Use the normal distribution to get the 2-sided probability content associated to n$\\sigma$)."
      ],
      "metadata": {
        "id": "p6hwXPpNV--5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xxx = ''"
      ],
      "metadata": {
        "id": "m7PCXDRvXxTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nsig = [1, 2, 3, 4, 5]\n",
        "\n",
        "bounds = eval(xxx)\n",
        "\n",
        "x2_1 = scipy.stats.chi2(1).ppf(bounds)\n",
        "x2_2 = scipy.stats.chi2(2).ppf(bounds)\n",
        "\n",
        "data = np.array(np.round([ nsig, bounds, x2_1, x2_2], 2)).T\n",
        "labels = ['sigmas', 'CL', '1 dof', '2 dof' ]\n",
        "pd.DataFrame(data, columns=labels)"
      ],
      "metadata": {
        "id": "GzxqpoyUWJi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 1 degree of freedom, the \"n$\\sigma$\" quantile of the $\\chi^2$ is simply $n^2$."
      ],
      "metadata": {
        "id": "RUS6kxqSYHmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Playing with quantiles</font>\n",
        "\n",
        "1. Plot the quantiles of a normal distribution ${\\cal N}(\\mu, \\sigma)$ versus the standard normal ${\\cal N}(0, 1)$ for $(\\mu, \\sigma)=\\left\\{(0,1), (2,1), (0,1/2)\\right\\}$\n"
      ],
      "metadata": {
        "id": "ieWDdyUPhd7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-12\n",
        "p = np.linspace(0+EPS, 1-EPS, 100)\n",
        "Q = lambda mu, s: scipy.stats.norm(loc=mu, scale=s).ppf(p)"
      ],
      "metadata": {
        "id": "995plM6Hdjmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the code of `xxx1`, `xxx2` and `xxx3` for the next cell."
      ],
      "metadata": {
        "id": "YfiD8hiUP_VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 12\n",
        "xxx1 = ''\n",
        "xxx2 = ''\n",
        "xxx3 = ''"
      ],
      "metadata": {
        "id": "4V4yEkzgPQ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Q(0,1), eval(xxx1), label=r\"$\\mu=0$, $\\sigma=1$\")\n",
        "plt.plot(Q(0,1), eval(xxx2), label=r\"$\\mu=2$, $\\sigma=1$\")\n",
        "plt.plot(Q(0,1), eval(xxx3), label=r\"$\\mu=0$, $\\sigma=1/2$\")\n",
        "plt.xlabel('Standard normal quantiles: $Q_X(p,\\mu=0, \\sigma=1)$')\n",
        "plt.ylabel('$Q_X(p,\\mu, \\sigma)$')\n",
        "plt.legend(facecolor='w')\n",
        "plt.axis([min(Q(0,1)), max(Q(0,1)), min(Q(0,1)), max(Q(0,1))]);"
      ],
      "metadata": {
        "id": "LAVzj-xdPMSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, '; '.join([xxx1, xxx2, xxx3]))"
      ],
      "metadata": {
        "id": "Eemu7Dq1PijJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Plot the poisson distribution quantiles vs. the normal approximation to this distribution (simply use the $\\mu=\\lambda$ and $\\sigma=\\sqrt{\\lambda}$) for $\\lambda=1, 10, 30, 100$"
      ],
      "metadata": {
        "id": "5WQpuW1Rmwoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 13\n",
        "xxx = ''\n",
        "yyy = ''"
      ],
      "metadata": {
        "id": "HVV1lnY3QXEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  { run: \"auto\", vertical-output: true }\n",
        "\n",
        "EPS = 1e-12\n",
        "p = np.linspace(0+EPS, 1-EPS, 1000)\n",
        "lb = 1\n",
        "# lb = 11 #@param {type:\"slider\", min:1, max:101, step:10}\n",
        "q = eval(xxx)\n",
        "plt.plot(q, q, 'k--', label='normal approxmation')\n",
        "plt.plot(q, eval(yyy), label='poisson')\n",
        "xmin = ymin = min(q)\n",
        "xmax = ymax = max(q)\n",
        "plt.axis([xmin, xmax, ymin, ymax]);"
      ],
      "metadata": {
        "id": "GD7FIRyZ8RB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, '; '.join([xxx, yyy]))"
      ],
      "metadata": {
        "id": "Zd2uEwsGzSM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title complete plotting { vertical-output: true }\n",
        "#@markdown Illustration of 1, 2, 3 and 5$\\sigma$ central and symmetric intervals with decreasing widths of black lines\n",
        "def plotting_EX(ax, lb):\n",
        "  norm = scipy.stats.norm\n",
        "  n = 1_000\n",
        "  p = lambda s: np.linspace(norm.cdf(-s), norm.cdf(s), n)\n",
        "  \n",
        "  # plotting normal quantiles\n",
        "  Qn = lambda mu, s: norm(mu, np.sqrt(mu)).ppf(p(s))\n",
        "  for s, ls, w in zip([   5,    3,    2,    1], # sigmas\n",
        "                      ['k:', 'k-', 'k-', 'k-'], # line styles\n",
        "                      [   1,    1,    5,   10]  # line widths\n",
        "                      ):\n",
        "    ax.plot(Qn(lb, s), Qn(lb, s), ls, lw=w)\n",
        "  \n",
        "  # plotting poisson quantiles\n",
        "  Qp = lambda mu, s: scipy.stats.poisson(mu).ppf(p(s))\n",
        "  ax.plot(Qn(lb, 5), Qp(lb, 5))\n",
        "  \n",
        "  ax.set_title(f\"QQ-plot for $\\lambda={lb:d}$\", fontsize=10)\n",
        "  ax.set_xlabel(f'${{\\\\cal N}}({lb},\\\\sqrt{{{lb}}})$')\n",
        "  ax.set_ylabel(r'${\\cal P}(\\lambda)$')\n",
        "  list_xlabels = np.round(norm(lb,np.sqrt(lb)).ppf(norm(0,1).cdf([-5, -3, -1, 0, 1, 3, 5])),0)\n",
        "  ax.set_xticks(list_xlabels)\n",
        "  ax.set_xticklabels(np.int32(list_xlabels), fontdict={'fontsize': 8})\n",
        "  ax.set_yticks(list_xlabels)\n",
        "  ax.set_yticklabels(np.int32(list_xlabels), fontdict={'fontsize': 8})\n",
        "  pass\n",
        "\n",
        "fig, ax = plt.subplots(2, 2, figsize=(8, 5))\n",
        "plt.subplots_adjust(wspace = 0.4, hspace = 0.6)\n",
        "for ax, lb in zip(ax.flatten(), [1, 10, 30, 100]):\n",
        "  plotting_EX(ax, lb)\n"
      ],
      "metadata": {
        "id": "mX7811MjnCBF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title $\\chi^2$ vs. Normal { run: \"auto\", vertical-output: true }\n",
        "ndof = 74 #@param {type:\"slider\", min:1, max:500, step:1}\n",
        "\n",
        "EPS = 1e-12\n",
        "xp = np.linspace(0+EPS, 1-EPS, 1000)\n",
        "q = scipy.stats.norm(ndof, np.sqrt(2*ndof)).ppf(xp)\n",
        "plt.plot(q, q, 'k--')\n",
        "norm = scipy.stats.norm\n",
        "n = 1_000\n",
        "p = lambda s: np.linspace(norm.cdf(-s), norm.cdf(s), n)\n",
        "# plotting normal quantiles\n",
        "Qn = lambda mu, s: norm(ndof, np.sqrt(2*ndof)).ppf(p(s))\n",
        "for s, ls, w in zip([   5,    3,    2,    1], # sigmas\n",
        "                    ['k:', 'k-', 'k-', 'k-'], # line styles\n",
        "                    [   1,    1,    5,   10]  # line widths\n",
        "                    ):\n",
        "  plt.plot(Qn(lb, s), Qn(lb, s), ls, lw=w)\n",
        "\n",
        "plt.plot(q, scipy.stats.chi2(ndof).ppf(xp))\n",
        "xmin = ymin = min(q)\n",
        "xmax = ymax = max(q)\n",
        "plt.axis([xmin, xmax, ymin, ymax]);"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kmQ6L1D40OxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Transforming a random variable</font>\n",
        "\n",
        "Let us assume $Y = \\varphi(X)$, with $\\varphi$, some function. We know the PDF of $X$, $f_X$. \n",
        "\n",
        "**Question**: What is the PDF of $Y$, $f_Y$?\n",
        "\n",
        "**Answer**:\n",
        "We use the CDF, which are usual functions (beware, PDF are densities, and carry imformation about density change in variable transformation while CDF act as standard mathematical function in variable transformation).\n",
        "\n",
        "Conservation of probabilties in a change of variable $y=\\varphi(x)$:\n",
        "\n",
        "$$f_X(x)\\;dx = f_{Y}(y)\\;dy$$\n",
        "\n",
        "or\n",
        "\n",
        "$$F_X(x) = F_{Y}(y)$$\n",
        "\n",
        "$F_Y(y) = {\\mathbb P}(Y<y) = {\\mathbb P}(\\varphi(X)<y)$\n",
        "\n",
        "if $\\varphi$ is invertible, and we note $\\varphi^{-1}$ it's inverse, we have:\n",
        "$F_Y(y) = {\\mathbb P}(Y<y) = {\\mathbb P}(X<\\varphi^{-1}(y))=F_{X}(\\varphi^{-1}(y))$\n",
        "\n",
        "Then $$f_{Y}(y) = \\frac{\\partial F_X({\\varphi^{-1}(y))}}{\\partial y} = \\frac{f_X(\\varphi^{-1}(y))}{\\varphi^{\\prime}(\\varphi^{-1}(y))}$$\n",
        "\n",
        "Taking back the example of yesterday: if $X$ follows a normal distribution with mean 0 and variance 1, what is the PDF of $Y=e^X$?"
      ],
      "metadata": {
        "id": "4nPYeLJdFk7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*ANSWER*: ..."
      ],
      "metadata": {
        "id": "iFUHExFYFk7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 14\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "mRDTwkpmFk70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fY(y):\n",
        "  return eval(xxx)"
      ],
      "metadata": {
        "id": "CAh6s0YzFk70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "CQF9lSEaFk70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.linspace(0+1e-12, 10, 1_000)\n",
        "plt.plot(y, fY(y), label='$e^{X}$ with X normal(0,1)')\n",
        "plt.plot(y, scipy.stats.lognorm.pdf(y,1), 'k--', label='log-normal ditribution')\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('$f_Y(y)$')\n",
        "plt.legend(facecolor='white');"
      ],
      "metadata": {
        "id": "fZxnLGZBFk70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: if the $\\varphi: x\\mapsto y$ transformation is not monotonic (monotonic means either increasing or decreasing, not both), you have to split up the interval in $x$ into subintervals where $\\varphi$ would be monotonic and define:\n",
        "\n",
        " $$f_{Y}(y) = \\sum_{\\varphi(x)=y}\\frac{f_X(x)}{\\varphi^{\\prime}(x)}$$\n",
        "\n",
        " A typical example is $Y=X^2$ where $\\varphi(x)=x^2$.\n",
        "\n",
        "  $$f_{Y}(y) = \\sum_{\\varphi(x)=y}\\frac{f_X(x)}{\\varphi^{\\prime}(x)} = \\frac{1}{2\\sqrt{y}}\\left(f_X\\left(\\sqrt{y}\\right) + f_X\\left(-\\sqrt{y}\\right)\\right)$$ \n"
      ],
      "metadata": {
        "id": "QiprIKOGFk70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DPggYYXgD23y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<a name=\"sampling\"></a>\n",
        "\n",
        "#<font color=\"darkred\">**II. Sampling**</font> [ðŸ”¼](#index)\n",
        "\n",
        "<font color=\"chocolate\">*Objectives:*</font> Drawing random variables from a given distribution.\n",
        "\n",
        "Drawing random variables from a given distribution\n",
        "Sampling = generating a random sample from a given specified distribution.\n",
        "First we use available ones. Then we can define our owns.\n",
        "This is useful for the field called Monte Carlo simulations."
      ],
      "metadata": {
        "id": "GGKC_Xfb3vC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea: you can check many statistical problems and approximate complex simulations with simple sampling and histogramming/fitting.\n",
        "\n",
        "You can for instance check approximations and properties of standard distributions:\n",
        "- binomials: sums, same p, and not., compounding binomials = binomial.\n",
        "- poisson: sum of poissons, but what about the difference of poisson random variables? \n",
        "- compounding a binomial with a poisson is poisson.\n",
        "- poisson = approx binomial large N small p, finite lambda = Np (can be small) ex: PMTs photostatistics dark noise etc.\n",
        "- Behaviour of Poisson, binomial, for large Np\n",
        "- Normal mu sigma and standard normal copmarison. $X=\\mu+\\sigma Z$ where $Z$ is standard normal.\n",
        "- CLT: Central Limit Theorem\n",
        "- Chi2 ($\\chi^2(n)$) as sum of squared $n$ standard normal.\n",
        "- Student as a standard normal over $\\sqrt{\\chi^2/n}$\n",
        "- Check transformation of random variables as we have already seen.\n",
        "- Check a fitting engine, and estimator distributoins\n",
        "- ...\n"
      ],
      "metadata": {
        "id": "S0144PkEHI2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Standard distributions</font>"
      ],
      "metadata": {
        "id": "Ixkc-YtM7POy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find how to generate 1,000 values of a normal random variable with mean=3 and sigma=1."
      ],
      "metadata": {
        "id": "krpB1wLyzwwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 15\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "oHwFYU73Tfnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xvals = eval(xxx)\n",
        "plt.hist(xvals, bins=50, range=(-3,9), color='tab:red', edgecolor='darkred');"
      ],
      "metadata": {
        "id": "6BJGXY6f5Ii-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "RUh1dslYT-es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy0fEsTwPgUw"
      },
      "source": [
        "1. Using the binomial distribution, plot a histogram of a sample of the number of successes in 100 experiments of 10 trials with 80% of sucess each.\n",
        "\n",
        "Notice: for a better reading of the plot, be careful with the bin centering, bin edges, \n",
        "bin width..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 16\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "m9-VBRF6Ug-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlzixneLIM2V"
      },
      "source": [
        "n_trial = 10\n",
        "success_proba = 0.8\n",
        "B = eval(xxx)\n",
        "n_experiments = 100\n",
        "\n",
        "Brvs = B.rvs(n_experiments)\n",
        "\n",
        "plt.hist(Brvs,bins=range(0,12), color='tab:red', edgecolor='darkred', align='left', rwidth=.5)\n",
        "plt.xlabel('Number of successes in 10 trials')\n",
        "plt.ylabel('Success counts with %d experiments' % n_experiments)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkBC2gnR4pX"
      },
      "source": [
        "2. Give the mean, variance, skewness and kurtosis of a binomial with n=20 trials and success probability p=0.4. You just need 1 line of code for all this. How?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 17\n",
        "xxx  = \"\""
      ],
      "metadata": {
        "id": "oAafZMPKUslX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r739NX9VSF03"
      },
      "source": [
        "B = scipy.stats.binom(n_trial, success_proba)\n",
        "\n",
        "# Computing summary statistics one by one\n",
        "print('Bino mean = %.2f, var= %.2f, std = %.2f' % (B.mean(), B.var(), B.std()))\n",
        "# Computing several summary statistics at once\n",
        "Bmean, Bvar, Bskew, Bkurt = eval(xxx)\n",
        "print('Bino mean = %.2f, var= %.2f, skew =%.2f, kurt = %.2f' % \\\n",
        "      (Bmean, Bvar, Bskew, Bkurt))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "BuyjHLELU0ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScWkffgTqit"
      },
      "source": [
        "## <font color=\"darkred\">Custom distribution - howtos</font>\n",
        "\n",
        "You would like to define your own PDF?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP9-nDT-2Qy5"
      },
      "source": [
        "# A completely arbitrary (but positive) function\n",
        "def my_custom_pdf(x, p):\n",
        "  return np.power(x, p)\n",
        "# Let's check its behaviour on some values\n",
        "x = np.linspace(0,5,1000)\n",
        "plt.plot(x, my_custom_pdf(x, 1))\n",
        "plt.plot(x, my_custom_pdf(x, 2))\n",
        "plt.title('Not normalized functions for PDFs', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The question is how to sample from it?\n",
        "\n",
        "Suppose $X$ follows a CDF $F_X$, whichi is continuous and strictly increasing. Then $F_X^{-1}$ is well defined as:\n",
        "\n",
        "$F^{-1}(u) = x$ if and only if $F(x)=u$ for $0<u<1$.\n",
        "\n",
        "Then de CDF of $Y=F_X(X)$ at $u\\in[0;1]$ is:\n",
        "\n",
        "$$F_{Y}(u)={\\mathbb P}(F_{X}(X)\\leqslant u)={\\mathbb P}(X\\leqslant F_{X}^{-1}(u))=F_X(F_X^{-1}(u))=u$$\n",
        "\n",
        "Then \n",
        "\n",
        "$f_Y(u)=1$ if $0<u<1$ and $0$ otherwise. So $F_{X}(X)$ is uniformly distributed. Thus if we take a uniform random varialbe $U$ on $[0;1]$, $F_X^{-1}(U)$ is distributed according to $f_X$."
      ],
      "metadata": {
        "id": "rmTrC7LFYEuz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwhkUrIO7LxD"
      },
      "source": [
        "# Let's generate random samples from them. How?\n",
        "from scipy.interpolate import interp1d \n",
        "x = np.linspace(0, 4, 1000)\n",
        "\n",
        "# Quick and dirty quantile definition\n",
        "par1 = 1\n",
        "fx = my_custom_pdf(x, par1) # not normalized...\n",
        "Fx = np.cumsum(fx)/np.sum(fx) # Computing the CDF\n",
        "Qx = interp1d(Fx, x, kind='linear')\n",
        "p = scipy.stats.uniform.rvs(size= 10_000)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,2))\n",
        "ax1.hist(Qx(p), bins=50, density=True)\n",
        "ax1.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax1.set_title(f'Distribution for p=%.0f' % par1, fontsize=12)\n",
        "\n",
        "par2 = 2\n",
        "fx = my_custom_pdf(x, par2)\n",
        "Fx = np.cumsum(fx)/np.sum(fx)\n",
        "Qx = interp1d(Fx, x, kind='linear')\n",
        "\n",
        "ax2.hist(Qx(p), bins=50, density=True)\n",
        "ax2.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax2.set_title(f'Distribution for p=%.0f' % par2, fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByZ7W4Xyzt0Z"
      },
      "source": [
        "You can even imagine to get a extremely regular sample. Instead of drawing random values for $p$ from the uniform distribution in $[0;1]$ you can use a equally spaced array for $p$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm2XZaOzzCMl"
      },
      "source": [
        "nsample = 200\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,2))\n",
        "p = np.linspace(0+1e-4, 1-1e-4, nsample)\n",
        "ax1.hist(Qx(p), bins=20, density=True)\n",
        "ax1.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax1.set_title('Equally spaced p', fontsize=12)\n",
        "\n",
        "np.random.seed(seed=123456)\n",
        "p = scipy.stats.uniform.rvs(size= nsample)\n",
        "ax2.hist(Qx(p), bins=20, density=True)\n",
        "ax2.plot(x, np.array(fx)/np.sum(fx[0:-1]*np.diff(x)), color='k')\n",
        "ax2.set_title('p from random uniform', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KanMa8f10Kao"
      },
      "source": [
        "This kind of trick can be used to check calculations with random variables with lower size samples without fluctuations for \"debugging purposes\"..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Making use of the quantile function for binning</font>"
      ],
      "metadata": {
        "id": "qMi5RD3p6VC-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysc0zcmbDYnu"
      },
      "source": [
        "X = scipy.stats.norm\n",
        "\n",
        "r = X.rvs(size = 100000)\n",
        "nbins = 20\n",
        "p = np.linspace(0+1e-5,1-1e-5,nbins+1)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3))\n",
        "plt.subplots_adjust(wspace=.3, top=0.8)\n",
        "ax1.hist(r, bins=X.ppf(p), color='tab:red', edgecolor='darkred', density=False, alpha=1, \\\n",
        "         label='Quantile bins')\n",
        "xmin, xmax = X.ppf([min(p), max(p)])\n",
        "bins_eq = np.linspace(xmin, xmax, nbins+1)\n",
        "ax1.hist(r, bins=bins_eq, color='tab:blue', edgecolor='none', density=False, alpha=.5, \\\n",
        "         label='Regular bins')\n",
        "ax1.set_title('Frequency histogram', fontsize=10)\n",
        "ax1.legend(facecolor='w')\n",
        "ax1.set_ylabel('Counts')\n",
        "ax1.set_xlabel('x')\n",
        "\n",
        "ax2.hist(r, bins=X.ppf(p), color='tab:red', edgecolor='darkred', density=True, alpha=1)\n",
        "xmin, xmax = X.ppf([min(p), max(p)])\n",
        "bins_eq = np.linspace(xmin, xmax, nbins+1)\n",
        "ax2.hist(r, bins=bins_eq, color='tab:red', edgecolor='darkred', density=True, alpha=.2)\n",
        "x = np.linspace(xmin,xmax,1000)\n",
        "ax2.plot(x, X.pdf(x), lw=3, ls='--', color='k')\n",
        "ax2.set_title('Density histogram', fontsize=10)\n",
        "ax2.set_ylabel('Counts/bin width')\n",
        "ax2.set_xlabel('x')\n",
        "fig.suptitle('Different binning definitons: impact of uneven binning on '+ \\\n",
        "              'type of histogram', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the quantile (inverse cumulative distribution) to define the bin widths, our collected data sample should look flat with a frequency binning. This means that every bin is expected to have the same statistical fluctuations. It could reveal convenient for graphical check up and for fitting (homeskedastic, i.e. same variance). And we can recover the probability density shape by drawing the density histogram. Both definitions are very convenient."
      ],
      "metadata": {
        "id": "chfYkkaz8OkI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TLNrDnq-KQ6"
      },
      "source": [
        "From the PDF and the sample of size 20, Draw the sample quantile as a function of the theoretical quantile. [QQplot] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu = 1\n",
        "sigma = .5\n",
        "X = scipy.stats.norm(loc=mu, scale=sigma)\n",
        "n = 20\n",
        "r = X.rvs(size = n)"
      ],
      "metadata": {
        "id": "2vJdxvuX9B5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The observed quantiles `Qobs` are simply the ordered observed values of the random sample."
      ],
      "metadata": {
        "id": "DL0nHSgEcRwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 18\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "rFe0xVVWcd7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qobs = eval(xxx)"
      ],
      "metadata": {
        "id": "0x4eIOa_cElx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. One of the simplest definition of the theoretical quantiles are the the quantiles of the probability density taken at the corresponding fraction of the observed value with respect to the full sample size: the associated theoretical quantile to $x_{(i)}$ it $Q_X\\left(\\frac{i}{n}\\right)$"
      ],
      "metadata": {
        "id": "gZV6kHkNchKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 19\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "lRLGELlQdatE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qth = eval(xxx)"
      ],
      "metadata": {
        "id": "CHaROUeMcFS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "eqDtbAQndjo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantile variance**\n",
        "\n",
        "We have a sample $(x_1,\\ldots,x_n)$ of values from a random variable $X$. For simplicity in notations we assume the sample is ordered in increasing values of $x_i$: $x_1<x_2<\\ldots<x_k<\\ldots<x_n$.\n",
        "\n",
        "We define $p_k=F_X(x_k)$, then we have\n",
        "$x_k = F_X^{-1}(p_k)$. We want to know the error $\\delta x_k$ on $x_k$:\n",
        "\n",
        "$$\\delta x_k = \\delta F_X^{-1}(p_k) = F_X^{-1}(p_k^{\\star}+\\delta p_k) -  F_X^{-1}(p_k^{\\star})$$\n",
        "\n",
        "where $p_k^{\\star}$ is the true value and $\\delta p_k$ is the error on $p_k$ due to the finite sampling size of the sample.\n",
        "\n",
        "Then \n",
        "\n",
        "$$F_X^{-1}(p_k^{\\star}+\\delta p_k) -  F_X^{-1}(p_k^{\\star})\\simeq \\frac{1}{f_X\\left(F_X^{-1}(p^\\star_k)\\right)} \\delta p_k$$\n",
        "\n",
        "Thus to first order, we have\n",
        "\n",
        "$${\\rm Var}\\left[\\delta x_k\\right] \\simeq \\left(\\frac{1}{f_X\\left(F_X^{-1}(p^\\star_k)\\right)}\\right)^2 {\\rm Var}\\left[\\delta p_k\\right]$$\n",
        "\n",
        "The error on $p_k$ is the difference between the estimated $\\hat{p}_k$ and the true $p_k^{\\star}$: $\\delta p_k = \\hat{p}_k-p^\\star_k$. And $\\hat{p}_k$ is simply the number of $x_i$ values such that $x_i<x_k$ divided by the total number of $x_i$ values: $\\hat{p}_k = k/n$. \n",
        "\n",
        "For a given $x$ value, the number $k$ of $x_k$ below $x$ is a random variable. It's a binomial random variable ${\\cal B}(n, p^\\star)$. Then we have:\n",
        "\n",
        "$${\\rm Var}\\left[\\hat{p}\\right] = {\\rm Var}\\left[\\delta p+p^\\star\\right] = {\\rm Var}\\left[\\delta p\\right] = \\frac{1}{n^2}{\\rm Var}\\left[k\\right]=\\frac{np^\\star(1-p^\\star)}{n^2}=\\frac{p^\\star(1-p^\\star)}{n}$$\n",
        "\n",
        "Thus, we get the following estimate of $\\delta x_k$:\n",
        "\n",
        "$$\\delta x_k \\simeq \\frac{\\sqrt{p^\\star(1-p^\\star)}}{\\sqrt{n} \\times f_X\\left(F_X^{-1}(p^\\star)\\right)}$$"
      ],
      "metadata": {
        "id": "NvYrWALTDCiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QQvar(X,p,n):\n",
        "  # Variance approximation of the quantiles\n",
        "  return p*(1-p)/(n*(X.pdf(X.ppf(p)))**2)"
      ],
      "metadata": {
        "id": "-9bw2gQo9NK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('QQvar = ', QQvar(X,[.2 .3],10))\n",
        "std = np.asarray([ np.sqrt(QQvar(X,(i+1)/(n+1),len(r))) for i, _ in enumerate(r)])"
      ],
      "metadata": {
        "id": "anZ0We5N9OGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Qth, Qobs, linewidth=3, label=\"sample quantiles\")\n",
        "plt.plot(Qth, Qth, 'k', linestyle='-', linewidth=1, label=\"expected relation\")\n",
        "plt.plot(Qth, Qth+2*std, 'k', linestyle=':', linewidth=1, label=\"2Ïƒ CI\")\n",
        "plt.plot(Qth, Qth-2*std, 'k', linestyle=':', linewidth=1)\n",
        "plt.xlabel('Theoretical distribution quantiles')\n",
        "plt.ylabel('sample distribution quantiles')\n",
        "plt.legend(facecolor='w');"
      ],
      "metadata": {
        "id": "w54-bm3g9Tq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDri-7x-5lt"
      },
      "source": [
        "A QQplot is a convenient way to check a sample is following a given theoretical prediction. Here we check the sample is following a normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a simple Monte Carlo simulation to compute the error band otbained through error propagation method above.\n",
        "\n"
      ],
      "metadata": {
        "id": "a-8mqJy5EBLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 20\n",
        "nsim = 10\n",
        "\n",
        "# Put the simulation code here.\n",
        "\n"
      ],
      "metadata": {
        "id": "NOoCi0l4EO6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Qth, Qobs, linewidth=3, label=\"sample quantiles\")\n",
        "plt.plot(Qth,Qth, 'k', linestyle='-', linewidth=1, label=\"expected relation\")\n",
        "plt.plot(Qth,Qth+2*std, 'k', linestyle=':', linewidth=1, label=\"2Ïƒ CI approx\")\n",
        "plt.plot(Qth,Qth-2*std, 'k', linestyle=':', linewidth=1)\n",
        "plt.plot(Qth,Qth+2*std_MC, 'b', linestyle='--', linewidth=1, label=\"2Ïƒ CI through MC\")\n",
        "plt.plot(Qth,Qth-2*std_MC, 'b', linestyle='--', linewidth=1)\n",
        "plt.xlabel('Theoretical distribution quantiles')\n",
        "plt.ylabel('sample distribution quantiles')\n",
        "plt.legend(facecolor='w');"
      ],
      "metadata": {
        "id": "hEiJw-atHCnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you are confident, please submit by executing the cell below."
      ],
      "metadata": {
        "id": "bNSa6BwVcF6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _ii)"
      ],
      "metadata": {
        "id": "rlQ2Ap57Hcy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our Taylor expansion approximation to compute the standard error of the quantiles is very good in this case. This interval gives a quick uncertainty band to assess if the empirical distribution from the sample is following the theoretical prediction."
      ],
      "metadata": {
        "id": "N7Yzn0xGIWBG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wico17fS2Pg6"
      },
      "source": [
        "## <font color=\"darkred\">Averages and CLT, the Central Limit Theorem</font>\n",
        "\n",
        "**Central Limit Theorem in short**: For any distribution with finite variance, adding up a large amount or independent random variables $X_i$ with the same distribution (with finite variance) lead to a normal distribution of the sum of the variables $\\sum_{i=1}^N X_i$. This is the central limit theorem (CLT).\n",
        "\n",
        "We demonstrate below two cases: the coin flip of the dice roll. You can experiment choosing in the menu below and run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "x9FtKvafFZIm"
      },
      "source": [
        "experiment = \"coin\" #@param [\"coin\", \"dice\"]\n",
        "\n",
        "numIterations = np.asarray([1, 2, 5, 10, 50, 100]); #number of i.i.d RVs\n",
        "\n",
        "maxNumForExperiment = {'dice': 6, 'coin': 2} #max numbers represented on dice or coins\n",
        "nSamp=10000\n",
        "\n",
        "k = maxNumForExperiment[experiment]\n",
        "\n",
        "fig, fig_axes = plt.subplots(ncols=3, nrows=2, constrained_layout=True)\n",
        "\n",
        "for i,N in enumerate(numIterations):\n",
        "    y = np.random.randint(low=1, high=k+1, size=(N,nSamp)).sum(axis=0)\n",
        "    row = i//3\n",
        "    col=i%3\n",
        "    bins=np.arange(start=min(y), stop=max(y)+2, step=1)\n",
        "    fig_axes[row, col].hist(y, bins=bins, density=True, align='left')\n",
        "    plural = lambda n: 's' if n>1 else ''\n",
        "    fig_axes[row, col].set_title('N={} {}'.format(N,experiment+plural(N)), fontsize=10)\n",
        "    x = np.linspace(min(y)-2, max(y)+2, 100)\n",
        "    fig_axes[row, col].plot(x, scipy.stats.norm.pdf(x, loc=np.mean(y),\\\n",
        "                                              scale=np.std(y)),\\\n",
        "                            color='k', linewidth=1, linestyle='--')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2BseSdOa_KH"
      },
      "source": [
        "An important consequence is the convergence of the average:\n",
        "* as $1/\\sqrt{n}$ (random dispersions of the mean decrease with $n$ as $1/\\sqrt{n}$) if each component of the sum has a finite variance;\n",
        "* and to a normal distribution (CLT, see above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qp_4MoEHimr",
        "cellView": "form"
      },
      "source": [
        "#@title CLT convergence with $1/\\sqrt{n}$\n",
        "y = np.random.randint(low=1, high=7, size=(200,1)).cumsum(axis=0)\n",
        "ymean = y[-1]/len(y)\n",
        "\n",
        "plt.plot([S/(i+1) for i, S in enumerate(y)])\n",
        "plt.plot(ymean + .5/np.sqrt(np.arange(1,len(y)+1)),\\\n",
        "         color='k', linestyle='--', linewidth=1)\n",
        "plt.plot(ymean - .5/np.sqrt(np.arange(1,len(y)+1)),\\\n",
        "         color='k', linestyle='--', linewidth=1)\n",
        "plt.title(r'Convergence of sample average $\\propto\\frac{1}{\\sqrt{n}}$ ', fontsize=12)\n",
        "plt.xlabel('n')\n",
        "plt.ylabel(r'$\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$')\n",
        "plt.show()\n",
        "# for i, S in enumerate(y):\n",
        "#   print('i= %d, S= %.2f' % (i,S/(i+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "<a name=\"inference\"></a>\n",
        "\n",
        "#<font color=\"darkred\">**III. Statistical inference**</font> [ðŸ”¼](#index)\n",
        "If we define the probability we can sample from it. This is a sampling distribution. If the sampling distribution is fully known (no free parameters), we can check its consistency with the data. Otherwise we have to find a way to specify the unknowns.\n",
        "\n"
      ],
      "metadata": {
        "id": "bSmoe52BTUIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually the sampling distribution is known up to some parameters. Like Poisson distribution depends on a parameter, $\\lambda$, in the following way: $${\\cal P}(n;\\lambda)=\\frac{\\lambda^n}{n!}e^{-\\lambda}$$"
      ],
      "metadata": {
        "id": "v95P9LC9VffW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The natural question is when we observe some $n$ then called $n_{\\rm obs}$ which value of $\\lambda$ is the most likely to have produce this observation.\n",
        "\n",
        "For instance, if we have $n_{\\rm obs}=[1, 3, 2, 5, 3, 4, 1, 7]$ and if we assume this sample was drawn from a Poisson distribution ${\\cal P}(n;\\lambda)$, among the possible values of the $\\lambda$ parameter, which one is the most likely given these observed data? This is the type of question we can answer in statistical inference."
      ],
      "metadata": {
        "id": "fixmibRHwii2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">The Maximum Likelihood Estimator (MLE)</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "JIAMlql5TZj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum likelihood is a method of estimating the parameters of an assumed probability distribution $P(x;\\theta)$, given some observed data $x_{\\rm obs}$.\n",
        "\n",
        "The best estimation, $\\hat{\\theta}$, of unknown $\\theta$ is achieved by maximizing a likelihood function ${\\cal L}(\\theta) = P(x_{\\text{obs}}; \\theta)$ so that, under the assumed statistical model, the observed data is most probable:\n",
        "\n",
        "$$\\hat{\\theta} = \\underset{\\theta}{\\arg\\max} \\, {\\cal L}(\\theta),$$\n",
        "where $\\hat{\\theta}$ is the value of $\\theta$ which maximizes $\\cal L(\\theta)$.\n"
      ],
      "metadata": {
        "id": "Xovx_dMmsEX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Fitting an exponential distribution</font>\n",
        "\n",
        "Here the goal is to fit data with likelihood. And to pay attention to the likelihood normalization. We will be fitting an exponential distribution and an exponential distribution with cuts...\n",
        "\n",
        "\n",
        "\n",
        "### <font color=\"darkred\">Part 1</font>\n",
        "\n",
        "Generate a sample of size 100 from an exponential distribution $f_X(x,\\lambda) = \\lambda e^{-\\lambda x}$ with $\\lambda = 1/2$. Find for this, the appropriate function in `scipy.stats` package. Call `X` the variable which contains this data sample.\n",
        "\n",
        "1. Check that the mean and the variance are indeed from what you expect from an exponential distribution. Did you pay attention to the arguments to call this function such as `loc` and `scale`?\n",
        "\n",
        "2. Draw a histogram of this sample `X` (Use `matplotlib.pyplot.hist`).\n",
        "\n",
        "3. Get used to `bins`, `range`, `width`, `density` options. Try them and check the behaviour. Especially look at the y-axis scale.\n",
        "\n",
        "4. Plot superimposed the histogram and the exponential PDF from the function definition. How can you make the PDF and the histogram look more alike for what concerns the y-scale?"
      ],
      "metadata": {
        "id": "IAxmeAfBhkwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 21\n",
        "xxx = '' # be careful with expon definition"
      ],
      "metadata": {
        "id": "tPJL_A8yeVHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0YAV5PYR_8k"
      },
      "source": [
        "lb = .5\n",
        "X = eval(xxx)\n",
        "print('mean = %.2f' % np.mean(X))\n",
        "print('var = %.2f' % np.var(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "dCrvesTQcaPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1ndAP0bKchDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following bin edges [0, 1, 2, 5, 7, 10], write the command to plot a density histogram which should be comparable with the probability density. There is especially an option to provide related to the density status of the histogram."
      ],
      "metadata": {
        "id": "Iio2hwmJsCIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 22\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "mYq5MBilsVPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_edges = (0, 1, 2, 5, 7, 10)\n",
        "eval(xxx)\n",
        "plt.plot(x, scipy.stats.expon(loc=0,scale=1/lb).pdf(x));"
      ],
      "metadata": {
        "id": "vQX9yn7liuEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "V8KL1MpVsbAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaA68Xd6Sw_J"
      },
      "source": [
        "### <font color=\"darkred\">Part 2</font>\n",
        "\n",
        "We want now to fit the data set `X` we defined in the previous Question.\n",
        "You will have to use `minimize` function from `scipy.optimize`. Read the documentation.\n",
        "\n",
        "You have to define:\n",
        "1. the PDF of the exponential PDF ($f_X(x;\\lambda)$)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBjXA5SGSpRS"
      },
      "source": [
        "Qid = 23\n",
        "# Put the code here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _i)"
      ],
      "metadata": {
        "id": "vdAUvC8ws4Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. the loglikelihood ($-2\\log f_X(x;\\lambda)$) of the dataset with this PDF.\n",
        "\n"
      ],
      "metadata": {
        "id": "pW5NpadLsqvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 24\n",
        "# Put the code here\n",
        "\n"
      ],
      "metadata": {
        "id": "H_RUwHQPsk-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _i)"
      ],
      "metadata": {
        "id": "BQ3CYxMLs-gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then minimize the loglikelihood and plot the result (the histogram with the data and the best fit PDF i.e. the PDF with parameter value obtained from the minimization process of the loglikelihood).\n"
      ],
      "metadata": {
        "id": "iyWpvo85suwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 25\n",
        "xxx = ''"
      ],
      "metadata": {
        "id": "iSsxi-hBtEc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMWIGjeYS8x3"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "soln = eval(xxx)\n",
        "soln"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, xxx)"
      ],
      "metadata": {
        "id": "A1eBhqRwtQwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the log likelihood and a parabolic approximation close to the minimum."
      ],
      "metadata": {
        "id": "xxJN3eu4JpHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!id = 26 \n",
        "lb = np.linspace(0.1, .9, 100)\n",
        "# put the code here\n",
        "\n"
      ],
      "metadata": {
        "id": "7oF4-EfwuGik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _i)"
      ],
      "metadata": {
        "id": "x57YcvAWc1fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have a parabolic approximation we have ${\\ell(\\lambda)}\\simeq\\left(\\frac{\\lambda-\\hat{\\lambda}}{\\sigma_{\\lambda}}\\right)^2$\n",
        "\n",
        "When $\\lambda$ is $\\sigma_\\lambda$ away of $\\hat{\\lambda}$ then $\\ell(\\lambda=\\hat{\\lambda}+\\sigma_\\lambda) = \\ell(\\hat{\\lambda})+1$. For $2\\sigma_\\lambda$ it's $\\ell(\\lambda) = \\ell(\\hat{\\lambda})+4$ and so on."
      ],
      "metadata": {
        "id": "LKIYQa8aKPxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the $\\sigma_\\lambda$ using `scipy.optimize.fsolve`"
      ],
      "metadata": {
        "id": "z-wzHZUALVgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import fsolve\n",
        "from scipy.interpolate import interp1d\n",
        "loglik = interp1d(lb, [llexp(lb)-llexp(soln.x[0]) for lb in lb], kind='cubic',\n",
        "                  bounds_error= False, fill_value='extrapolate')"
      ],
      "metadata": {
        "id": "3Q4EFMlrLn-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qid = 27\n",
        "# Put the code here"
      ],
      "metadata": {
        "id": "bKxIkq15dEhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb = np.linspace(0.1, .9, 100)\n",
        "plt.plot(lb, [llexp(lb)-llexp(soln.x[0]) for lb in lb]);\n",
        "plt.plot(soln.x[0], 0, 'ko')\n",
        "plt.plot(lb, [(lb-soln.x[0])**2/(2*soln.hess_inv.todense()[0]) for lb in lb]);\n",
        "plt.axis([.4, .6, -1, 10])"
      ],
      "metadata": {
        "id": "6eVr2sfANycg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _ii)"
      ],
      "metadata": {
        "id": "bMhFxVv5dKG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gENbsZDTBmo"
      },
      "source": [
        "### <font color=\"darkred\">Part 3</font>\n",
        "\n",
        "Assume now that `X` is still from the exponential distribution with $\\lambda=1/2$ defined above, but for some experimental reason you only have access to data bewteen 1 and 5.\n",
        "\n",
        "1. Use the first question instructions to generate a sample of `X` values of size 1000. Then select only the cases between 1 and 5. For instance use \n",
        "`X[(X>1)&(X<5)]`. \n",
        "2. Fit this new dataset with the likelihood defined in previous question. Do you find the same estimate? Why?\n",
        "3. You would have expected do get the same estimate of $\\lambda$ since this is the same generating process. What biases the estimator?\n",
        "4. Check the normalization of the likelihood on the sample space ($x$ values). The likelihood should always be normalized to 1 on the sample space. Define therefore a new likelihood correctly normalized for $x$ values between 1 and 5. Do you get now a reasonable estimator of $\\lambda$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD3Xz32DS-oH"
      },
      "source": [
        "Y = X[(X>1)&(X<5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtrIGPtmTHtb"
      },
      "source": [
        "plt.hist(Y, bins=50, range=(0,10));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNU3dHLTJv6"
      },
      "source": [
        "Qid = 28\n",
        "#put your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _i)"
      ],
      "metadata": {
        "id": "YeRHoPAzdkHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay attention to the normalization of the likelihood in the sample space."
      ],
      "metadata": {
        "id": "l3fh1GxudsAe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unk_WA9cTMpz"
      },
      "source": [
        "Qid = 29\n",
        "# Put your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit(Qid, _i)"
      ],
      "metadata": {
        "id": "yEW6Xyogdz8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the 2 solutions: the non normalized log likelihood and the normalized one."
      ],
      "metadata": {
        "id": "W4C8FDA3d68n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5c0KYFxVpqP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a significant difference when the likelihood is not properly normalized on sample space. Beware of this recurrent trap! Always use correctly normalized probabilities, especially when the normalization factor depends on the parameter to be fitted."
      ],
      "metadata": {
        "id": "3St-KWhuza36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Binning the data</font>\n",
        "\n",
        "From the previous example of the exponential distribution, you can try to bin the sample.\n",
        "\n",
        "Each bin should follow a Poisson distribution given by integral content of the PDF of $X$ random variable (the exponential) over each bin.\n",
        "\n",
        "Fit the binned sample with a a Poisson log likelihood for each bin.\n",
        "\n",
        "Add a flat background.\n",
        "\n",
        "Profile out the background: minimize with respect to background total number\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCrcBhyIM9mG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"darkred\">Chi square $\\chi^2$</font>\n",
        "\n",
        "In the large sample case, distributions can often be well approximated by a normal distribution (thanks to the Central Limit Theorem). The likelihood can then be approximated by\n",
        "$$\n",
        "{\\cal L}(\\theta) = \\prod_{i=1}^{n_{\\text{bins}}} {\\cal P}(n_i, \\mu_i(\\theta)) \\approx \\prod_{i=1}^{n_{\\text{bins}}} \\frac{1}{\\sqrt{2\\pi}\\sigma_i}\\exp\\left(-\\frac{1}{2} \\frac{(n_i - \\mu_i(\\theta))^2}{\\sigma_i^2} \\right)\n",
        "$$\n",
        "\n",
        "And then taking -2 log of the likelihood\n",
        "\n",
        "$$\n",
        "{\\ell}(\\theta) = -2 \\log {\\cal L}(\\theta) = \\sum_{i=1}^{n_{\\text{bins}}}\\left(\\frac{n_i - \\mu_i(\\theta)}{\\sigma_i}\\right)^2 + {\\rm constant}\n",
        "$$\n",
        "which is just the least squares. When each $n_i$ follows a normal distribution of mean $\\mu_i(\\theta)$ and variance $\\sigma_i^2$, then $\\ell(\\theta)$ follows a $\\chi^2$ with $n_{\\rm bins}$ degrees of freedom.\n",
        "\n",
        "This is an important motivation for using $\\ell=-2\\log{\\cal L}$ : it can be defined for any likelihood, but in the normal case, it reduces to the $\\chi^2$.\n",
        "\n",
        "Wilk's theorem:\n",
        "\n",
        "When the sample size is large, the Wilk's theorem state that $\\ell=-2\\log{\\cal L}$ should follow a $\\chi^2$ distribution. This theorem is often used to obtain quickly asymptotic confidence intervals with $\\chi^2$ distribution.\n",
        "\n",
        "A remark concerning the degrees of freedom: we have a number of bins and a number of fitted parameters. The minimum chi square $\\chi^2_{\\rm min}$ provides model adequacy to data, also referred as Goodness of Fit (GoF): follows a $\\chi^2$ with $N_{\\rm bins}-N_{\\rm par}$ degrees of freedom. $\\Delta\\chi^2$ provides a specific model best fit with respect to all possible values of the parametsrs. $\\Delta\\chi^2$ follows a $\\chi^2$ distribution with $N_{\\rm par}$ degrees of freedom."
      ],
      "metadata": {
        "id": "rT89hvY0TcYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the sample size is large, the Wilk's theorem state that $\\ell=-2\\log{\\cal L}$ should follow a $\\chi^2$ distribution. This theorem id often used to obatin quickly asymptotic confidence intervals. $\\chi^2$ distribution. Degrees of freedom. Number of bins. Number of fitted parameters. $\\chi^2_{\\rm min}$ and $\\Delta\\chi^2$. $\\chi^2_{\\rm min}$ provides model adequacy to data, also referred as Goodness of Fit (GoF): follows a $\\chi^2$ with $N_{\\rm bins}-N_{\\rm par}$ degrees of freedom. $\\Delta\\chi^2$ provides a specific model best fit with respect to all possible values of the parametsrs. $\\Delta\\chi^2$ follows a $\\chi^2$ distribution with $N_{\\rm par}$ degrees of freedom."
      ],
      "metadata": {
        "id": "BHwppqM6N9Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UcNcc1LPVnXz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}